{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb8af3c",
   "metadata": {},
   "source": [
    "(a) Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinySories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\n",
    "\n",
    "Deliverable: A one-to-two sentence response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3253c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_bytes=22502601\n",
      "token_count=5461210\n",
      "bytes_per_token=4.120442\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from cs336_basics.Tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_files(\n",
    "    \"vocab.json\",\n",
    "    \"merges.json\",\n",
    "    special_tokens=[\"<|endoftext|>\"]\n",
    ")\n",
    "\n",
    "path = Path(\"../data/TinyStoriesV2-GPT4-valid.txt\")\n",
    "raw_bytes = path.stat().st_size\n",
    "\n",
    "token_count = 0\n",
    "with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for chunk in f:\n",
    "        token_count += len(tokenizer.encode(chunk))\n",
    "\n",
    "print(f\"raw_bytes={raw_bytes}\")\n",
    "print(f\"token_count={token_count}\")\n",
    "print(f\"bytes_per_token={raw_bytes / token_count:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e375984",
   "metadata": {},
   "source": [
    "(b) What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.\n",
    "\n",
    "Deliverable: A one-to-two sentence response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbfd0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_bytes=289998753\n",
      "token_count=91369966\n",
      "bytes_per_token=3.173896\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from cs336_basics.Tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_files(\n",
    "    \"vocab.json\",\n",
    "    \"merges.json\",\n",
    "    special_tokens=[\"<|endoftext|>\"]\n",
    ")\n",
    "\n",
    "path = Path(\"../data/owt_valid.txt\")\n",
    "raw_bytes = path.stat().st_size\n",
    "\n",
    "token_count = 0\n",
    "with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for chunk in f:\n",
    "        token_count += len(tokenizer.encode(chunk))\n",
    "\n",
    "print(f\"raw_bytes={raw_bytes}\")\n",
    "print(f\"token_count={token_count}\")\n",
    "print(f\"bytes_per_token={raw_bytes / token_count:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6139a",
   "metadata": {},
   "source": [
    "(c) Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)?\n",
    "\n",
    "Deliverable: A one-to-two sentence response.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "- From the earlier owt_valid run I measured 289,998,753 bytes processed in about 137 s, so throughput ≈2.1×10⁶ bytes/s (≈2.1 MB/s).\n",
    "- The Pile is roughly 825 GB ≈8.9×10¹¹ bytes. At 2.1 MB/s, total time is 8.9×10¹¹ ÷ 2.1×10⁶ ≈4.0×10⁵ s, or about 112 hours—just under 4¾ days of continuous tokenization.\n",
    "- Even using the slightly faster TinyStories timing (~2.5 MB/s) the job still lands around 4 days, so planning for roughly 4–5 days of wall-clock time is reasonable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845705d5",
   "metadata": {},
   "source": [
    "(d) Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We’ll use this later to train our language model. We recommend serializing the token IDs as a NumPy array of datatype uint16. Why is uint16 an appropriate choice?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
